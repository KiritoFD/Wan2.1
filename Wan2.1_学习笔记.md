# Wan2.1 项目学习笔记

## 1. 项目概述

Wan2.1 是一个开源的大规模视频生成模型套件，由阿里巴巴 Wan 团队开发。它是一个多功能的基础模型，专注于视频生成领域，具有以下主要特点：

- **开源性能领先**：在多项基准测试中，Wan2.1 的性能超过了现有的开源模型和最先进的商业解决方案。
- **支持消费级 GPU**：T2V-1.3B 模型只需要 8.19 GB 显存，在 RTX 4090 上约 4 分钟即可生成 5 秒钟的 480P 视频。
- **多任务支持**：包括文本到视频(T2V)、图像到视频(I2V)、视频编辑、文本到图像(T2I)以及视频到音频转换等功能。
- **视觉文本生成**：是第一个能够生成中英文文本的视频模型，提高了实际应用中的文本表现能力。
- **高效视频 VAE**：Wan-VAE 提供了高效的视频编解码能力，可以处理任意长度的 1080P 视频而保持时序信息。

## 1.1 系统结构示意图

```
                        +-------------------+
                        |    Wan2.1 系统    |
                        +-------------------+
                                 |
              +------------------+------------------+
              |                  |                  |
    +---------v---------+ +------v-------+ +--------v--------+
    |   文本到视频(T2V)   | | 图像到视频(I2V) | |  文本到图像(T2I) |
    +---------+---------+ +------+-------+ +--------+--------+
              |                  |                  |
              v                  v                  v
    +---------+---------+ +------+-------+ +--------+--------+
    |    提示词扩展       | |   提示词扩展   | |    提示词扩展    |
    +---------+---------+ +------+-------+ +--------+--------+
              |                  |                  |
              +------------------+------------------+
                                 |
                        +--------v--------+
                        |    输入编码      |
                        | T5/CLIP/条件处理 |
                        +--------+--------+
                                 |
                        +--------v--------+
                        |    扩散模型      |
                        |  WanModel/采样器 |
                        +--------+--------+
                                 |
                        +--------v--------+
                        |    视频VAE      |
                        |  编码器/解码器   |
                        +--------+--------+
                                 |
                        +--------v--------+
                        |    生成输出      |
                        | 视频/图像/批处理 |
                        +-----------------+
```

## 2. 核心概念解析

### 2.1 整体架构

Wan2.1 基于扩散变换器（Diffusion Transformer，DiT）范式设计，通过一系列创新实现了生成能力的显著提升：

1. **时空变分自编码器**（Spatio-temporal Variational Autoencoder，VAE）
2. **可扩展的训练策略**
3. **大规模数据构建**
4. **自动评估指标**

### 2.2 核心组件

#### 2.2.1 Wan-VAE（3D 变分自编码器）

VAE（Variational Autoencoder，变分自编码器）是一种生成模型，用于学习数据的潜在表示并生成新数据。Wan-VAE 是专为视频生成设计的 3D 因果 VAE 架构：

- **时空压缩**：提高了视频数据的压缩效率
- **内存使用优化**：减少了处理过程中的内存消耗
- **时序因果性**：确保了生成视频的时序连贯性
- **无限长度处理**：支持编解码无限长度的 1080P 视频，不丢失历史时序信息

VAE 的主要组件包括：
- `Encoder3d`：将输入视频编码为潜在表示
- `Decoder3d`：将潜在表示解码回视频
- `CausalConv3d`：因果 3D 卷积，确保时序信息的正确传递
- `ResidualBlock`：残差块，增强特征提取能力
- `AttentionBlock`：注意力块，关注视频中的重要内容

#### 2.2.2 视频扩散变换器（DiT）

Wan2.1 使用流匹配（Flow Matching）框架，通过以下组件实现高质量视频生成：

- **T5 编码器**：编码多语言文本输入
- **交叉注意力机制**：在每个变换器块中嵌入文本信息
- **时间嵌入处理**：使用 MLP（带有 Linear 层和 SiLU 层）处理输入的时间嵌入
- **模块化参数预测**：单独预测六个调制参数，提高性能

DiT 模型规格：
- 1.3B 参数版本：维度 1536，头数 12，层数 30
- 14B 参数版本：维度 5120，头数 40，层数 40

### 2.3 生成过程

Wan2.1 支持两种主要的生成模式：文本到视频（T2V）和图像到视频（I2V）。

#### 文本到视频（T2V）流程

1. **文本编码**：使用 T5 编码器将输入提示转换为嵌入表示
2. **噪声初始化**：生成初始随机噪声
3. **扩散采样**：通过多步去噪过程逐渐从噪声生成视频帧
   - 支持两种采样器：UniPC 和 DPM++
   - 使用无分类器引导（Classifier-free guidance）控制生成
4. **视频解码**：将潜在表示通过 VAE 解码器转换为最终视频

#### 图像到视频（I2V）流程

1. **图像编码**：使用 CLIP 模型和 VAE 编码输入图像
2. **文本编码**：处理输入提示
3. **条件扩散**：基于图像和文本条件进行扩散采样
4. **视频解码**：生成与输入图像风格一致的视频序列

## 3. 代码结构详解

整个项目组织为以下主要模块：

### 3.1 主要目录

```
Wan2.1/
├── generate.py              # 主要的生成入口脚本
├── wan/
│   ├── text2video.py        # 文本到视频生成类
│   ├── image2video.py       # 图像到视频生成类
│   ├── modules/             # 核心模型组件
│   │   ├── vae.py           # 视频 VAE 实现
│   │   ├── model.py         # 主要模型实现
│   │   ├── clip.py          # CLIP 模型实现
│   │   ├── t5.py            # T5 文本编码器
│   │   └── attention.py     # 注意力机制实现
│   ├── distributed/         # 分布式训练和推理
│   │   └── xdit_context_parallel.py # 并行上下文处理
│   ├── utils/               # 实用工具
│   │   ├── fm_solvers.py    # 流匹配求解器
│   │   └── prompt_extend.py # 提示词扩展工具
│   └── configs/             # 配置文件
├── gradio/                  # Gradio Web 界面
└── tests/                   # 测试脚本
```

### 3.2 核心类和函数

#### 3.2.1 生成器类

1. **WanT2V（文本到视频）**
   - 初始化核心组件（T5 编码器、VAE、DiT 模型）
   - `generate()` 方法处理完整的生成过程

2. **WanI2V（图像到视频）**
   - 额外整合了 CLIP 模型处理输入图像
   - 生成与输入图像风格一致的视频序列

#### 3.2.2 关键组件

1. **WanVAE**
   - 封装了视频编解码功能
   - `encode()` 将视频转换为潜在表示
   - `decode()` 将潜在表示转换回视频帧

2. **WanModel**
   - 核心扩散变换器模型
   - 整合文本和时间信息进行条件生成

3. **采样器**
   - `FlowUniPCMultistepScheduler`：UniPC 采样器
   - `FlowDPMSolverMultistepScheduler`：DPM++ 采样器
   - 控制扩散过程的分步去噪

## 4. 特殊技术点分析

### 4.1 提示词扩展 (Prompt Extension)

为了增强生成视频的质量，Wan2.1 引入了提示词扩展机制：

- **远程 API 方式**：使用 Dashscope API（阿里云）
- **本地模型方式**：使用 Qwen 模型本地扩展
- 提示词扩展极大丰富了生成细节，提高了视频质量和场景连贯性

### 4.2 分布式推理与内存优化

为了支持大型模型的高效推理，项目实现了多种优化策略：

- **FSDP（完全分片数据并行）**：跨多个 GPU 分片模型
- **USP（统一序列并行）**：处理长序列数据
- **模型卸载**：`offload_model` 选项支持将模型暂时迁移到 CPU 以节省 GPU 内存
- **混合精度计算**：使用 `torch.cuda.amp` 进行混合精度推理

### 4.3 流匹配扩散框架

与传统扩散模型相比，Wan2.1 使用流匹配框架有以下优势：

- **更高效的采样**：更快的收敛速度
- **更好的时序控制**：通过 shift 参数调节生成视频的时序动态
- **高质量生成**：在相同步数下产生更高质量的视频

## 5. 实践使用指南

### 5.1 命令行参数解析

使用 `generate.py` 脚本的关键参数：

- `--task`：任务类型（t2v-14B、i2v-14B、t2i-14B、t2v-1.3B）
- `--size`：输出分辨率（如 1280*720、832*480）
- `--ckpt_dir`：模型检查点目录
- `--prompt`：生成提示词
- `--image`：I2V 任务的输入图像
- `--sample_solver`：采样器选择（unipc、dpm++）
- `--sample_steps`：采样步数（T2V 默认 50，I2V 默认 40）
- `--sample_shift`：流匹配时间参数（一般为 5.0，480P 时为 3.0）
- `--sample_guide_scale`：无分类器引导尺度（默认 5.0，1.3B 模型推荐 6.0）
- `--use_prompt_extend`：是否启用提示词扩展
- `--dit_fsdp`、`--t5_fsdp`：是否为 DiT 和 T5 启用 FSDP
- `--ulysses_size`：USP 并行大小
- `--offload_model`：是否将模型卸载到 CPU 以节省 GPU 内存

### 5.2 模型选择指南

- **大型生产项目**：使用 14B 参数模型获取最佳质量
- **消费级设备**：1.3B 模型可在单个 RTX 4090 上运行
- **图像到视频**：根据分辨率需求选择对应的 I2V 模型
- **高分辨率需求**：使用多 GPU 配置和 FSDP+USP 优化

### 5.3 常见问题解决

1. **显存不足（OOM）**：
   - 启用 `--offload_model True` 和 `--t5_cpu`
   - 降低分辨率
   - 使用较小的模型（1.3B 替代 14B）

2. **生成质量不佳**：
   - 启用提示词扩展 `--use_prompt_extend`
   - 调整 `--sample_shift` 参数（影响时间动态）
   - 增加 `--sample_steps` 提高质量
   - 调整 `--sample_guide_scale`（更高值增强提示词依从性）

3. **生成速度慢**：
   - 使用多 GPU 并行推理
   - 尝试第三方加速工具如 TeaCache（可提速约 2 倍）
   - 使用 Diffusers 集成版本

## 6. 扩展与深入研究方向

1. **模型微调**：使用特定领域数据微调模型以适应特定应用场景
2. **集成应用**：将 Wan2.1 集成到创意工具、内容平台或游戏引擎
3. **速度优化**：探索更多量化和模型压缩技术
4. **创意控制**：开发更精确的生成控制机制

## 7. 总结

Wan2.1 代表了视频生成领域的重要进展，通过开源大规模视频生成模型，降低了创意内容创作的门槛。该项目的核心创新包括：

1. 针对视频特性设计的 3D 因果 VAE
2. 扩散变换器架构的优化实现
3. 提示词扩展技术
4. 多种内存和计算优化策略

通过这些技术，Wan2.1 在保持开放可访问性的同时，提供了商业级视频生成能力。作为初学者，可以从简单的文本到视频生成开始，逐步探索更多高级功能和优化技术。